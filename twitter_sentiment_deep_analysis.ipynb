{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-sentiment-deep-analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNGaHIdYMmG3mSYNrM8cWyo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIibYfLqQcEK"
      },
      "source": [
        "**Remarks**\n",
        "\n",
        "In this notebook, we will learn about deep analysis in twitter-sentiment. Yeayy! You must review my notebook to gain more knowledge about what I was written. I used different method: Deep Learning, RNN, and LSTM. Let's check it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm9Gei7yQyVg"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rrhz0SbLQ3Y7"
      },
      "source": [
        "# import library\n",
        "# 1. dataframe environment\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "# 2. NLP tools\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "from textblob import TextBlob\n",
        "\n",
        "# 3. Modelling tools\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 4. Visualization tools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 5. Keras Environment\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D\n",
        "from keras.layers import MaxPooling1D, GlobalMaxPooling1D\n",
        "from keras import utils\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk9a-79cSYzm"
      },
      "source": [
        "# NLP Modelling and function\n",
        "nlp = en_core_web_lg.load()\n",
        "def get_wordcounts(x):\n",
        "  length = len(str(x).split())\n",
        "  return length\n",
        "\n",
        "def get_charcounts(x):\n",
        "  s = x.split()\n",
        "  x = ''.join(s)\n",
        "  return len(x)\n",
        "\n",
        "def get_avg_wordlength(x):\n",
        "\tcount = get_charcounts(x)/get_wordcounts(x)\n",
        "\treturn count\n",
        "\n",
        "def get_stopwords_counts(x):\n",
        "\tl = len([t for t in x.split() if t in stopwords])\n",
        "\treturn l\n",
        "\n",
        "def get_hashtag_counts(x):\n",
        "\tl = len([t for t in x.split() if t.startswith('#')])\n",
        "\treturn l\n",
        "\n",
        "def get_mentions_counts(x):\n",
        "\tl = len([t for t in x.split() if t.startswith('@')])\n",
        "\treturn l\n",
        "\n",
        "def get_digit_counts(x):\n",
        "\tdigits = re.findall(r'[0-9,.]+', x)\n",
        "\treturn len(digits)\n",
        "\n",
        "def get_digit_counts(x):\n",
        "\tdigits = re.findall(r'[0-9,.]+', x)\n",
        "\treturn len(digits)\n",
        "\n",
        "def get_uppercase_counts(x):\n",
        "  x = len([t for t in x.split() if t.isupper()])\n",
        "  return x\n",
        "\n",
        "def get_emails(x):\n",
        "\temails = re.findall(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+\\b)', x)\n",
        "\tcounts = len(emails)\n",
        "\n",
        "\treturn counts, emails\n",
        "\n",
        "\n",
        "def get_urls(x):\n",
        "\turls = re.findall(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', x)\n",
        "\tcounts = len(urls)\n",
        "\n",
        "\treturn counts, urls\n",
        "\n",
        "def make_base(x):\n",
        "\tx = str(x)\n",
        "\tx_list = []\n",
        "\tdoc = nlp(x)\n",
        "\t\n",
        "\tfor token in doc:\n",
        "\t\tlemma = token.lemma_\n",
        "\t\tif lemma == '-PRON-' or lemma == 'be':\n",
        "\t\t\tlemma = token.text\n",
        "\n",
        "\t\tx_list.append(lemma)\n",
        "\treturn ' '.join(x_list)\n",
        "\n",
        "def get_value_counts(df, col):\n",
        "\ttext = ' '.join(df[col])\n",
        "\ttext = text.split()\n",
        "\tfreq = pd.Series(text).value_counts()\n",
        "\treturn freq\n",
        "\n",
        "def spelling_correction(x):\n",
        "\tx = TextBlob(x).correct()\n",
        "\treturn x\n",
        "\n",
        "def get_basic_features(df):\n",
        "\tif type(df) == pd.core.frame.DataFrame:\n",
        "\t\tdf['char_counts'] = df['text'].apply(lambda x:get_charcounts(x))\n",
        "\t\tdf['word_counts'] = df['text'].apply(lambda x:get_wordcounts(x))\n",
        "\t\tdf['avg_wordlength'] = df['text'].apply(lambda x:get_avg_wordlength(x))\n",
        "\t\tdf['stopwords_counts'] = df['text'].apply(lambda x:get_stopwords_counts(x))\n",
        "\t\tdf['hashtag_counts'] = df['text'].apply(lambda x:get_hashtag_counts(x))\n",
        "\t\tdf['mentions_counts'] = df['text'].apply(lambda x:get_mentions_counts(x))\n",
        "\t\tdf['digits_counts'] = df['text'].apply(lambda x:get_digit_counts(x))\n",
        "\t\tdf['uppercase_counts'] = df['text'].apply(lambda x:get_uppercase_counts(x))\n",
        "\telse:\n",
        "\t\tprint('ERROR: This function takes only Pandas DataFrame')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u28dW8IAZSrr",
        "outputId": "1fe4d881-ef77-40aa-837b-11720464432f"
      },
      "source": [
        "# retrieving from Kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d cosmos98/twitter-and-reddit-sentimental-analysis-dataset\n",
        "!unzip \"twitter-and-reddit-sentimental-analysis-dataset.zip\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading twitter-and-reddit-sentimental-analysis-dataset.zip to /content\n",
            " 50% 5.00M/10.0M [00:00<00:00, 24.5MB/s]\n",
            "100% 10.0M/10.0M [00:00<00:00, 39.6MB/s]\n",
            "Archive:  twitter-and-reddit-sentimental-analysis-dataset.zip\n",
            "  inflating: Reddit_Data.csv         \n",
            "  inflating: Twitter_Data.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "kGg1UQSJZjvU",
        "outputId": "44ad9557-b210-49d2-b9e0-ea0b64d8aba7"
      },
      "source": [
        "df_tweet = pd.read_csv(\"/content/Twitter_Data.csv\")\n",
        "df_tweet.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when modi promised “minimum government maximum...</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>talk all the nonsense and continue all the dra...</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>asking his supporters prefix chowkidar their n...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer who among these the most powerful world...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          clean_text  category\n",
              "0  when modi promised “minimum government maximum...      -1.0\n",
              "1  talk all the nonsense and continue all the dra...       0.0\n",
              "2  what did just say vote for modi  welcome bjp t...       1.0\n",
              "3  asking his supporters prefix chowkidar their n...       1.0\n",
              "4  answer who among these the most powerful world...       1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvT9zQ72Z30I",
        "outputId": "938c55d6-3bea-4f0b-b247-5045b568c2a9"
      },
      "source": [
        "# checking null\n",
        "df_tweet.isnull().sum()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "clean_text    4\n",
              "category      7\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YGNRtLIZ-Ce"
      },
      "source": [
        "# dropping missing rows\n",
        "df_tweet.dropna(axis=0, inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDwdUOD2aE69",
        "outputId": "a118239d-0922-4292-e2cf-919c484d2ac1"
      },
      "source": [
        "# counting category\n",
        "df_tweet.category.value_counts()\n",
        "# NOTE:\n",
        "# 1 -- Positive\n",
        "# 0 -- Neutral\n",
        "# -1 -- Negative"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1.0    72249\n",
              " 0.0    55211\n",
              "-1.0    35509\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UpMqT4MamQ1",
        "outputId": "e1ab6fe3-7bc2-47da-aef0-9b713f776403"
      },
      "source": [
        "# splitting data\n",
        "y_tweet = df_tweet['category']\n",
        "X_tweet = df_tweet['clean_text']\n",
        "\n",
        "# tokenization\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(X_tweet)\n",
        "\n",
        "# unique words\n",
        "vocab_size = len(token.word_index)+1\n",
        "print(\"Vocab size: {}\".format(vocab_size))\n",
        "\n",
        "# encoded text to numerical\n",
        "encoded_text = token.texts_to_sequences(X_tweet)\n",
        "\n",
        "max_lenth=120\n",
        "X_tweet_final = pad_sequences(encoded_text,maxlen=max_lenth,padding='post')\n",
        "\n",
        "print(\"Dataframe \", X_tweet_final.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 113679\n",
            "Dataframe  (162969, 120)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "50wyIVA4Zts0",
        "outputId": "de32fce1-31b6-43f9-8dd5-b23158cd105e"
      },
      "source": [
        "df_red = pd.read_csv(\"/content/Reddit_Data.csv\")\n",
        "df_red.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>clean_comment</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>family mormon have never tried explain them t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>buddhism has very much lot compatible with chr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>seriously don say thing first all they won get...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what you have learned yours and only yours wha...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>for your own benefit you may want read living ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       clean_comment  category\n",
              "0   family mormon have never tried explain them t...         1\n",
              "1  buddhism has very much lot compatible with chr...         1\n",
              "2  seriously don say thing first all they won get...        -1\n",
              "3  what you have learned yours and only yours wha...         0\n",
              "4  for your own benefit you may want read living ...         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXS2SlHlZ1Yf",
        "outputId": "a4924ed2-a4a3-439b-cbcb-30e67b9c5e52"
      },
      "source": [
        "# checking null\n",
        "df_red.isnull().sum()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "clean_comment    100\n",
              "category           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPENMsrbaXuZ",
        "outputId": "25ad5efd-0022-4164-996e-8e97474740ef"
      },
      "source": [
        "# dropping missing rows\n",
        "df_red.dropna(axis=0, inplace=True)\n",
        "# counting category\n",
        "df_red.category.value_counts()\n",
        "# NOTE:\n",
        "# 1 -- Positive\n",
        "# 0 -- Neutral\n",
        "# -1 -- Negative"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              " 1    15830\n",
              " 0    13042\n",
              "-1     8277\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJlfbHTdaimV"
      },
      "source": [
        "**DEEP LEARNING**\n",
        "\n",
        "It is the first step analysis. Enjoy learn!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfp4Vuvsacxf"
      },
      "source": [
        "# train test split Data Tweet\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tweet_final, y_tweet, test_size=0.2, random_state=42, stratify=y_tweet)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPkxbOyNb_z5",
        "outputId": "4c2a66e5-013e-4fd7-cef0-1abce4bac9b1"
      },
      "source": [
        "# deeplearning model\n",
        "vec_size = 300\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, vec_size, input_length=120))\n",
        "\n",
        "# model 1\n",
        "model.add(Conv1D(32, 3, activation=\"relu\"))\n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model 2\n",
        "model.add(Conv1D(64, 3, activation=\"relu\"))\n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# model 3\n",
        "model.add(Conv1D(128, 3, activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# final\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 120, 300)          34103700  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 118, 32)           28832     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 39, 32)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 39, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 37, 64)            6208      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 12, 64)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 10, 128)           24704     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 34,163,573\n",
            "Trainable params: 34,163,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XsDXQbEckgK"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "%time\n",
        "history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kThsKnHzcuST"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(5)\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJfN6bobgaSM"
      },
      "source": [
        "**RNN Analysis**\n",
        "\n",
        "It is the second step analysis. Enjoy learn!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSlEvTIidm8Y"
      },
      "source": [
        "print(\"shape of X_train: \",X_train.shape)\n",
        "print(\"shape of X_test: \",X_test.shape)\n",
        "print(\"shape of y_train: \",y_train.shape)\n",
        "print(\"shape of y_test: \",y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVyWee54gtxP"
      },
      "source": [
        "vec_size = 300\n",
        "model = sequential()\n",
        "model.add(Embedding(vocab_size, vec_size, input_length=120))\n",
        "model.add(SimpleRNN(50, retun_sequences = False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "adam = optimizers.Adam(lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBQVE0dLhBeo"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyVsC3NChDX6"
      },
      "source": [
        "%time\n",
        "history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBjIcgAqhEwD"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(5)\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyqP33R6hOPm"
      },
      "source": [
        "**LSTM Analysis**\n",
        "\n",
        "It is the last step analysis, but not least. Enjoy learn!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioB6Cs0ZhQTl"
      },
      "source": [
        "# LSTM support library\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers, layers, losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqBufDrxh7Qq"
      },
      "source": [
        "vec_size = 300\n",
        "# layer 1\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, vec_size, input_length=120, embeddings_regularizer=regularizer.l2(0.005)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# layer 2\n",
        "model.add(LSTM(vec_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True,\n",
        "               kernel_regularizer=regularizers.l2(0.005), bias_regularizer=regularizers.l2(0.005)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# layer 3\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# layer 4\n",
        "model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001),\n",
        "                bias_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# last layer\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ummwm3bkOhB"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "%time\n",
        "history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRhFuZiUkSdH"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(5)\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}