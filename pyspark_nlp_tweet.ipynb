{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark-nlp-tweet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+dGJZtkKgib1PoN6I7mlv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojzcxviiMFId"
      },
      "source": [
        "**Remarks**\n",
        "\n",
        "In this notebook, we will learn about twitter-tweet using pyspark. Finally, we read again previous lesson to generate data from kaggle. Let's learning! NOTED: If you confused about how to download dataset in kaggle, you can try manually. Here's: [Sentiment Analysis](https://www.kaggle.com/kazanova/sentiment140)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmmcGRjkNZwd",
        "outputId": "da96fe99-1ff6-4d29-a91e-f28a1f9ba000"
      },
      "source": [
        "# installing pyspark\n",
        "!pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 60 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 55.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=e55ed301d8e246d30e5a432073788d3ee5d50cb9a01b359ba237c44ec5fc60ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0e8F6UqLe46"
      },
      "source": [
        "# import library\n",
        "# 1. pyspark environment\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression, DecisionTreeClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# 2. DataFrame environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import html\n",
        "\n",
        "# 3. NLP tools\n",
        "import spacy\n",
        "import re\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVChYiTUNUFc"
      },
      "source": [
        "# load data from kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NijUOU45OREI",
        "outputId": "0b2bb4b3-ce0e-499b-87f6-ad18a0376f33"
      },
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "!unzip sentiment140.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sentiment140.zip to /content\n",
            " 90% 73.0M/80.9M [00:00<00:00, 69.3MB/s]\n",
            "100% 80.9M/80.9M [00:00<00:00, 104MB/s] \n",
            "Archive:  sentiment140.zip\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izUQ_59POTwi"
      },
      "source": [
        "# creating spark session\n",
        "spark = SparkSession.builder.appName('tweet').getOrCreate()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3yBPPt0OdmW",
        "outputId": "0c68bb46-56e1-4332-b49f-e2a206314ca7"
      },
      "source": [
        "# reading dataset\n",
        "data = spark.read.csv('/content/training.1600000.processed.noemoticon.csv', inferSchema=True)\n",
        "data.printSchema()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: integer (nullable = true)\n",
            " |-- _c1: long (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v89yrT-qOnK5",
        "outputId": "568eb4c5-dad2-488d-d360-bac583f49406"
      },
      "source": [
        "# changing label name\n",
        "data = data.withColumnRenamed(\"_c0\", \"target\").withColumnRenamed(\"_c1\", \"id\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
        "data.limit(5).show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|target|        id|                date|    flag|           user|                text|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyAy_eCSPLdv",
        "outputId": "b6c7ef88-95b3-4f17-aca1-ec7a101a9699"
      },
      "source": [
        "# Showing target information\n",
        "data.groupBy(\"target\").count().show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------+\n",
            "|target| count|\n",
            "+------+------+\n",
            "|     4|800000|\n",
            "|     0|800000|\n",
            "+------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRGVE4TDPYuJ",
        "outputId": "fbf8519b-2958-4c3d-cc1f-d0e0a005ac38"
      },
      "source": [
        "# Checking null dataset\n",
        "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+----+----+----+----+\n",
            "|target| id|date|flag|user|text|\n",
            "+------+---+----+----+----+----+\n",
            "|     0|  0|   0|   0|   0|   0|\n",
            "+------+---+----+----+----+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHzl1Mf7P0pB"
      },
      "source": [
        "Dataset already used in this modelling, but we need to change target label (0,4) into (0,1).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_01oBsXPt60",
        "outputId": "1811324f-dd09-45c6-977e-7fe011dceb53"
      },
      "source": [
        "# changing target label\n",
        "data = data.withColumn(\"target\", when(data[\"target\"] == 4,1).otherwise(data[\"target\"]))\n",
        "data.groupBy(\"target\").count().show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------+\n",
            "|target| count|\n",
            "+------+------+\n",
            "|     1|800000|\n",
            "|     0|800000|\n",
            "+------+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDXle9IgQS2s"
      },
      "source": [
        "# NLP RULE STEP\n",
        "# 1. cleaning dataset\n",
        "data_clean = data.select('target', (lower(regexp_replace('text', \"[^a-zA-Z\\\\s]\", \"\")).alias('text')))\n",
        "\n",
        "# 2. tokenize text\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "data_token = tokenizer.transform(data_clean).select(\"target\", \"tokens\")\n",
        "\n",
        "# 3. remove stop-words\n",
        "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"clean_word\")\n",
        "data_stop = remover.transform(data_token).select(\"target\", \"clean_word\")\n",
        "\n",
        "# 4. stem-text\n",
        "stemmer = PorterStemmer()\n",
        "stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
        "data_stem = data_stop.withColumn(\"words_stemmed\", stemmer_udf(\"clean_word\")).select(\"target\", \"words_stemmed\")\n",
        "\n",
        "# 5. filter length word > 5\n",
        "filter_length_udf = udf(lambda row: [x for x in row if len(x) >= 3], ArrayType(StringType()))\n",
        "final_words = data_stem.withColumn(\"words\", filter_length_udf(col(\"words_stemmed\")))\n",
        "\n",
        "# 6. TF-IDF\n",
        "hashingTF = HashingTF(inputCol=\"words_stemmed\", outputCol=\"rawFeatures\")\n",
        "featurizedData = hashingTF.transform(final_words)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGOCmWTwYk9r"
      },
      "source": [
        "# Modelling step\n",
        "# 1. splitting data\n",
        "train, test = rescaledData.randomSplit([0.7, 0.3])\n",
        "\n",
        "# 2. Modelling NB\n",
        "nb = NaiveBayes(modelType=\"multinomial\",labelCol=\"target\", featuresCol=\"features\")\n",
        "nbModel = nb.fit(train)\n",
        "nb_predictions = nbModel.transform(test)\n",
        "# 2. Modelling LG\n",
        "lr = LogisticRegression(featuresCol = 'features', labelCol = 'target', maxIter=10)\n",
        "lrModel = lr.fit(train)\n",
        "lrPreds = lrModel.transform(test)\n",
        "# 2. Modelling DT\n",
        "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'target', maxDepth = 3)\n",
        "dtModel = dt.fit(train)\n",
        "dtPreds = dtModel.transform(test)\n",
        "# 2. Modelling RF\n",
        "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'target')\n",
        "rfModel = rf.fit(train)\n",
        "rfPreds = rfModel.transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYxFezfRajy3"
      },
      "source": [
        "# 3. Evaluation\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "nb_accuracy = evaluator.evaluate(nb_predictions)\n",
        "print(\"Accuracy of NaiveBayes is = %g\"% (nb_accuracy))\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "lr_accuracy = evaluator.evaluate(lrPreds)\n",
        "print(\"Accuracy of Logistic Regression is = %g\"% (lr_accuracy))\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "dt_accuracy = evaluator.evaluate(dtPreds)\n",
        "print(\"Accuracy of Decision Trees is = %g\"% (dt_accuracy))\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "rf_accuracy = evaluator.evaluate(rfPreds)\n",
        "print(\"Accuracy of Random Forests is = %g\"% (rf_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}